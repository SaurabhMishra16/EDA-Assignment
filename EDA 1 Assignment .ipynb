{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c81710c-6c49-480b-b1ad-a9e4de36c1d1",
   "metadata": {},
   "source": [
    "Q1. What are the key features of the wine quality data set? Discuss the importance of each feature in\n",
    "predicting the quality of wine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a090b36f-512b-4fe0-8512-72cc5e92be0b",
   "metadata": {},
   "source": [
    "The \"wine quality\" dataset typically refers to one of two commonly used datasets in machine learning: the Wine Quality (Red Wine) dataset and the Wine Quality (White Wine) dataset. These datasets are used to predict the quality of wine based on various chemical properties. Here are the key features of these datasets and their importance in predicting wine quality:\n",
    "\n",
    "**1. Fixed Acidity:** This feature represents the amount of non-volatile acids in the wine. These acids are important for the taste and stability of the wine. Too much or too little acidity can affect the wine's quality, so it's a crucial factor in predicting wine quality.\n",
    "\n",
    "**2. Volatile Acidity:** Volatile acidity refers to the presence of volatile acids in wine, primarily acetic acid. High levels of volatile acidity can lead to a vinegar-like taste, negatively impacting wine quality. Monitoring and controlling this acidity is essential.\n",
    "\n",
    "**3. Citric Acid:** Citric acid is one of the non-volatile acids found in wine. It can contribute to the wine's freshness and flavor. Its presence is generally desirable, but the amount needs to be balanced for optimal wine quality.\n",
    "\n",
    "**4. Residual Sugar:** Residual sugar is the amount of sugar left in the wine after fermentation. It can influence the wine's sweetness and body. The right balance of residual sugar is essential for achieving the desired taste profile in different types of wine.\n",
    "\n",
    "**5. Chlorides:** Chlorides in wine can come from various sources, including soil and grape varieties. Too much chloride can lead to a salty taste, negatively affecting the wine's overall quality. Maintaining an appropriate level is important.\n",
    "\n",
    "**6. Free Sulfur Dioxide:** Sulfur dioxide (SO2) is used in winemaking as a preservative and antioxidant. Monitoring the amount of free SO2 is crucial to prevent oxidation and spoilage of wine. It also influences the wine's aroma and taste.\n",
    "\n",
    "**7. Total Sulfur Dioxide:** This feature represents the total amount of sulfur dioxide in the wine, including both free and bound forms. High levels of total SO2 can have adverse effects on wine quality, including off-putting odors.\n",
    "\n",
    "**8. Density:** Density is a measure of the wine's mass per unit volume. It can be an indicator of the wine's alcohol content and sweetness. It's an important parameter for assessing wine quality.\n",
    "\n",
    "**9. pH:** pH measures the acidity or alkalinity of the wine. It influences the wine's taste and stability. Wines with the right pH level tend to be more balanced and of higher quality.\n",
    "\n",
    "**10. Sulphates:** Sulphates are a byproduct of fermentation and are present in wine. They can affect the wine's flavor and aroma. Proper levels of sulphates are necessary for quality wine production.\n",
    "\n",
    "**11. Alcohol:** Alcohol content is a key component of wine quality. It contributes to the wine's body and mouthfeel. The right alcohol level is essential for achieving the desired style and quality of wine.\n",
    "\n",
    "**12. Quality (Target Variable):** This is the target variable that indicates the perceived quality of the wine, usually on a scale from 1 to 10 (or 3 to 9 in some versions of the dataset). It's the variable you want to predict based on the other features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fc01ec-1b81-4541-b1b2-730c81193c00",
   "metadata": {},
   "source": [
    "Q2. How did you handle missing data in the wine quality data set during the feature engineering process?\n",
    "Discuss the advantages and disadvantages of different imputation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3aa5d2-54a1-4338-874c-26ea8bd81246",
   "metadata": {},
   "source": [
    "Handling missing data is a crucial step in the feature engineering process when working with datasets like the wine quality dataset. There are several techniques to handle missing data, each with its own advantages and disadvantages. Here's how missing data can be handled and the pros and cons of each method:\n",
    "\n",
    "**1. Removal of Rows (Listwise Deletion):**\n",
    "\n",
    "   - **Advantages:**\n",
    "     - Simple and straightforward.\n",
    "     - Preserves the original data distribution.\n",
    "   \n",
    "   - **Disadvantages:**\n",
    "     - Reduces the sample size, potentially leading to loss of information.\n",
    "     - If data is missing completely at random (MCAR), this method is unbiased. However, if data is missing at random (MAR) or not at random (MNAR), it can introduce bias.\n",
    "\n",
    "**2. Mean/Median/Mode Imputation:**\n",
    "\n",
    "   - **Advantages:**\n",
    "     - Simple and fast.\n",
    "     - Does not change the variable's distribution significantly.\n",
    "     - Applicable for both continuous and categorical data.\n",
    "\n",
    "   - **Disadvantages:**\n",
    "     - May not be suitable if missing data is not missing completely at random (MCAR).\n",
    "     - Can introduce bias, especially if the percentage of missing data is high.\n",
    "     - Reduces variance but doesn't account for uncertainty introduced by imputation.\n",
    "\n",
    "**3. Regression Imputation:**\n",
    "\n",
    "   - **Advantages:**\n",
    "     - Utilizes relationships between variables.\n",
    "     - Can provide more accurate imputations when there are strong relationships between variables.\n",
    "   \n",
    "   - **Disadvantages:**\n",
    "     - Assumes that the relationship between variables is linear, which may not always be the case.\n",
    "     - Sensitive to outliers.\n",
    "     - Complexity increases with the number of missing values and variables.\n",
    "\n",
    "**4. K-Nearest Neighbors (KNN) Imputation:**\n",
    "\n",
    "   - **Advantages:**\n",
    "     - Uses multiple neighboring data points for imputation, which can capture complex relationships.\n",
    "     - Can handle both continuous and categorical data.\n",
    "\n",
    "   - **Disadvantages:**\n",
    "     - Computationally intensive, especially for large datasets.\n",
    "     - Choice of the number of neighbors (k) can impact results.\n",
    "     - Sensitive to the distance metric used.\n",
    "\n",
    "**5. Multiple Imputation:**\n",
    "\n",
    "   - **Advantages:**\n",
    "     - Provides multiple imputed datasets, allowing for uncertainty estimation.\n",
    "     - Applicable to various types of missing data patterns (MCAR, MAR, MNAR).\n",
    "   \n",
    "   - **Disadvantages:**\n",
    "     - Requires more computational resources and may be time-consuming.\n",
    "     - Complex to implement.\n",
    "     - Final results require aggregation across multiple imputed datasets.\n",
    "\n",
    "**6. Predictive Modeling (e.g., Random Forests, Gradient Boosting):**\n",
    "\n",
    "   - **Advantages:**\n",
    "     - Utilizes the power of machine learning models to predict missing values.\n",
    "     - Can capture complex relationships between variables.\n",
    "   \n",
    "   - **Disadvantages:**\n",
    "     - Requires a significant amount of data to train a predictive model effectively.\n",
    "     - May overfit if not properly tuned.\n",
    "     - Computationally intensive.\n",
    "\n",
    "**7. Domain-Specific Imputation:**\n",
    "\n",
    "   - **Advantages:**\n",
    "     - Takes into account domain knowledge and expertise.\n",
    "     - Can lead to meaningful and accurate imputations in specific contexts.\n",
    "   \n",
    "   - **Disadvantages:**\n",
    "     - Highly dependent on the availability of domain knowledge.\n",
    "     - May not be suitable for all datasets or missing data patterns.\n",
    "\n",
    "The choice of imputation technique should be guided by the nature of the data, the percentage of missing data, and the underlying missing data mechanism (MCAR, MAR, MNAR). It's often a good practice to explore the data to understand the missingness pattern and consider a combination of techniques, such as multiple imputation or a combination of domain-specific and statistical imputation, to obtain robust and meaningful imputed values. Additionally, it's essential to evaluate the impact of missing data handling on the performance of your predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277c5b0-70f4-4c97-9371-c581e79bb189",
   "metadata": {},
   "source": [
    "Q3. What are the key factors that affect students' performance in exams? How would you go about\n",
    "analyzing these factors using statistical techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51daf3-4ef3-4c05-bd47-131cc078c2c4",
   "metadata": {},
   "source": [
    "Students' performance in exams can be influenced by a wide range of factors, both academic and non-academic. Analyzing these factors using statistical techniques in Python can provide valuable insights into which variables have a significant impact on student performance. Here's a general approach to analyzing these factors:\n",
    "\n",
    "**1. Data Collection:**\n",
    "   - Gather data on students' exam scores and relevant factors that might affect their performance. This data could include variables such as:\n",
    "     - Demographic information (e.g., age, gender, ethnicity).\n",
    "     - Socioeconomic status (e.g., parental education, income).\n",
    "     - Study habits (e.g., hours of study per week).\n",
    "     - Attendance in classes.\n",
    "     - Previous academic performance (e.g., GPA).\n",
    "     - Test preparation methods (e.g., tutoring, self-study).\n",
    "     - Psychological factors (e.g., motivation, stress levels).\n",
    "\n",
    "**2. Data Preprocessing:**\n",
    "   - Clean the data by handling missing values, outliers, and ensuring data consistency.\n",
    "   - Encode categorical variables if necessary (e.g., using one-hot encoding or label encoding).\n",
    "   - Normalize or standardize numerical variables to have them on the same scale.\n",
    "\n",
    "**3. Exploratory Data Analysis (EDA):**\n",
    "   - Conduct EDA to understand the data and relationships between variables.\n",
    "   - Use summary statistics, visualizations (e.g., histograms, scatter plots, box plots), and correlation matrices to identify patterns and potential associations.\n",
    "\n",
    "**4. Hypothesis Testing:**\n",
    "   - Formulate hypotheses about which factors are likely to affect students' exam performance.\n",
    "   - Perform statistical tests to evaluate these hypotheses. For example:\n",
    "     - t-tests or ANOVA to compare means between different groups (e.g., gender, ethnicity).\n",
    "     - Regression analysis to examine the relationship between continuous variables (e.g., study hours and exam scores).\n",
    "\n",
    "**5. Feature Selection:**\n",
    "   - Use techniques like feature selection algorithms (e.g., recursive feature elimination) or correlation analysis to identify the most relevant factors that significantly impact exam scores.\n",
    "\n",
    "**6. Machine Learning Models:**\n",
    "   - Build predictive models using machine learning algorithms (e.g., linear regression, decision trees, random forests, or gradient boosting).\n",
    "   - Split the data into training and testing sets to assess model performance.\n",
    "   - Use cross-validation to ensure robustness of the models.\n",
    "\n",
    "**7. Model Evaluation:**\n",
    "   - Evaluate the model's performance using appropriate metrics (e.g., mean squared error, R-squared for regression, accuracy, F1-score for classification).\n",
    "   - Analyze the model's feature importance to identify which factors are the strongest predictors of exam performance.\n",
    "\n",
    "**8. Interpretation:**\n",
    "   - Interpret the results to draw meaningful conclusions. Identify the factors that have the most significant impact on student performance.\n",
    "   - Consider the practical implications of the findings and their potential application for educational improvements.\n",
    "\n",
    "**9. Reporting and Visualization:**\n",
    "   - Present your findings in a clear and visually appealing manner using plots, tables, and narratives.\n",
    "   - Provide recommendations or insights based on the analysis.\n",
    "\n",
    "In Python, you can use libraries such as NumPy, pandas, Matplotlib, Seaborn, scikit-learn, and statsmodels to perform these tasks. Each step involves writing Python code to preprocess, analyze, and visualize the data, as well as to build and evaluate machine learning models if necessary.\n",
    "\n",
    "Remember that the specific analysis may vary depending on the dataset and research questions, and it's important to choose the appropriate statistical techniques and models accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858b62f-69d4-482f-96fc-ee5bca64cc9f",
   "metadata": {},
   "source": [
    "Q4. Describe the process of feature engineering in the context of the student performance data set. How\n",
    "did you select and transform the variables for your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc9716-c57f-449f-9891-ea233790b963",
   "metadata": {},
   "source": [
    "Feature engineering is a critical step in the data analysis and modeling process, where you create new features or modify existing ones to improve the performance of your machine learning models. In the context of a student performance dataset, here's a description of the feature engineering process and how variables are selected and transformed for modeling:\n",
    "\n",
    "**1. Data Understanding:**\n",
    "   - Begin by understanding the dataset and its variables. This includes studying the data dictionary and gaining insights into what each variable represents.\n",
    "\n",
    "**2. Initial Data Exploration:**\n",
    "   - Perform exploratory data analysis (EDA) to understand the relationships between variables and identify potential patterns.\n",
    "   - Use summary statistics, visualizations, and correlation matrices to gain insights into the data.\n",
    "\n",
    "**3. Feature Selection:**\n",
    "   - Identify which variables are likely to be relevant to predicting student performance. This can be based on domain knowledge and initial EDA.\n",
    "   - Consider factors like demographic information, socioeconomic status, study habits, attendance, and previous academic performance.\n",
    "   - Use statistical tests (e.g., t-tests, ANOVA) or feature selection techniques (e.g., recursive feature elimination) to quantify the importance of features.\n",
    "\n",
    "**4. Categorical Variable Encoding:**\n",
    "   - If the dataset includes categorical variables (e.g., gender, ethnicity), encode them into numerical format. Common methods include one-hot encoding or label encoding.\n",
    "   \n",
    "```python\n",
    "import pandas as pd\n",
    "student_data_encoded = pd.get_dummies(student_data, columns=['gender', 'ethnicity'])\n",
    "```\n",
    "\n",
    "**5. Handling Missing Data:**\n",
    "   - Address missing data in variables. Depending on the nature of the missingness (MCAR, MAR, MNAR), you might apply imputation techniques like mean imputation, regression imputation, or more advanced methods.\n",
    "   \n",
    "```python\n",
    "# Example of mean imputation for a variable\n",
    "mean_value = student_data['variable_name'].mean()\n",
    "student_data['variable_name'].fillna(mean_value, inplace=True)\n",
    "```\n",
    "\n",
    "**6. Feature Scaling:**\n",
    "   - Normalize or standardize numerical features if necessary. This ensures that features are on a similar scale, which can be important for some machine learning algorithms.\n",
    "   \n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "student_data['numerical_feature'] = scaler.fit_transform(student_data[['numerical_feature']])\n",
    "```\n",
    "\n",
    "**7. Feature Engineering:**\n",
    "   - Create new features or transform existing ones based on domain knowledge or insights from EDA. These features can capture specific relationships or patterns that may not be apparent in the original data. Examples might include:\n",
    "     - **Study Time Ratio:** The ratio of weekly study time to weekly travel time.\n",
    "     - **Parental Education Level:** Combining the education levels of both parents into a single variable.\n",
    "     - **Final Grade:** Aggregating individual exam scores into a single final grade.\n",
    "   \n",
    "```python\n",
    "# Example of creating a new feature\n",
    "student_data['study_time_ratio'] = student_data['studytime'] / student_data['traveltime']\n",
    "```\n",
    "\n",
    "**8. Feature Selection (Revisited):**\n",
    "   - Reevaluate the importance of features after encoding, imputation, and engineering. Use feature importance scores from machine learning models or domain knowledge to select the most relevant features.\n",
    "\n",
    "**9. Model Building:**\n",
    "   - Use the selected and transformed features to build predictive models. Depending on the problem (regression, classification), choose appropriate machine learning algorithms (e.g., linear regression, decision trees, logistic regression).\n",
    "\n",
    "**10. Model Evaluation:**\n",
    "    - Evaluate the model's performance using relevant metrics (e.g., mean squared error for regression, accuracy for classification).\n",
    "    - Analyze feature importance from the model to understand which variables have the most significant impact on student performance.\n",
    "\n",
    "**11. Iterative Process:**\n",
    "    - Feature engineering is often an iterative process. You may need to revisit and refine your features based on model performance and additional insights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7451ec6a-90ce-4f6a-8d5c-5a20e7e62e6e",
   "metadata": {},
   "source": [
    "Q5. Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distribution\n",
    "of each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to\n",
    "these features to improve normality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331cba0b-4e1c-4f8e-b599-15962f83342f",
   "metadata": {},
   "source": [
    "Q5. Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distribution\n",
    "of each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to\n",
    "these features to improve normality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f09145a-e130-4add-bb7c-2a9a5a7ad67c",
   "metadata": {},
   "source": [
    "To perform exploratory data analysis (EDA) on the wine quality dataset and identify the distribution of each feature, you can follow these steps using Python:\n",
    "\n",
    "**1. Load the Data:**\n",
    "   - First, you need to load the wine quality dataset. You can use libraries like pandas and seaborn to load and visualize the data.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset (assuming you have the dataset file)\n",
    "wine_data = pd.read_csv('wine_quality.csv')\n",
    "```\n",
    "\n",
    "**2. Summary Statistics:**\n",
    "   - Calculate and examine summary statistics for each feature, including mean, median, standard deviation, and quartiles, to get an initial sense of the data.\n",
    "\n",
    "```python\n",
    "summary_stats = wine_data.describe()\n",
    "print(summary_stats)\n",
    "```\n",
    "\n",
    "**3. Distribution Plots:**\n",
    "   - Create distribution plots (histograms) for each feature to visualize their distributions. This will help you identify non-normality.\n",
    "\n",
    "```python\n",
    "# Create histograms for all features\n",
    "plt.figure(figsize=(12, 8))\n",
    "for column in wine_data.columns:\n",
    "    sns.histplot(data=wine_data, x=column, kde=True, label=column)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**4. Identify Non-Normality:**\n",
    "   - Examine the histograms and consider the following to identify non-normality:\n",
    "     - Skewness: If the distribution is skewed to the left or right.\n",
    "     - Kurtosis: If the distribution has heavy tails or is excessively peaked.\n",
    "     - Outliers: Presence of outliers in the data.\n",
    "\n",
    "**5. Transformation Options:**\n",
    "\n",
    "Based on the histograms and summary statistics, you can identify features that exhibit non-normality. Some common transformations that can be applied to improve normality include:\n",
    "\n",
    "   - **Logarithmic Transformation:** Useful for reducing right-skewness.\n",
    "   ```python\n",
    "   wine_data['skewed_feature_log'] = np.log1p(wine_data['skewed_feature'])\n",
    "   ```\n",
    "\n",
    "   - **Box-Cox Transformation:** Useful for reducing skewness and making the distribution more normal.\n",
    "   ```python\n",
    "   from scipy.stats import boxcox\n",
    "   wine_data['skewed_feature_boxcox'], _ = boxcox(wine_data['skewed_feature'])\n",
    "   ```\n",
    "\n",
    "   - **Square Root Transformation:** Can be used for reducing right-skewness.\n",
    "   ```python\n",
    "   wine_data['skewed_feature_sqrt'] = np.sqrt(wine_data['skewed_feature'])\n",
    "   ```\n",
    "\n",
    "   - **Exponential Transformation:** Useful for reducing left-skewness.\n",
    "   ```python\n",
    "   wine_data['skewed_feature_exp'] = np.exp(wine_data['skewed_feature'])\n",
    "   ```\n",
    "\n",
    "   - **Yeo-Johnson Transformation:** Similar to Box-Cox but can handle both positive and negative values.\n",
    "   ```python\n",
    "   from scipy.stats import yeojohnson\n",
    "   wine_data['skewed_feature_yeo'], _ = yeojohnson(wine_data['skewed_feature'])\n",
    "   ```\n",
    "\n",
    "Choose the appropriate transformation based on the characteristics of the feature and the requirements of your analysis. After applying transformations, you can then reevaluate the distributions and check if they are closer to normality.\n",
    "\n",
    "Remember that not all features need to be transformed, and the decision should be made based on the specific goals of your analysis and the characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f353fb78-af1d-486a-bd6f-baf95c90526f",
   "metadata": {},
   "source": [
    "Q6. Using the wine quality data set, perform principal component analysis (PCA) to reduce the number of\n",
    "features. What is the minimum number of principal components required to explain 90% of the variance in\n",
    "the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d302060d-6bf3-4ef5-957b-46958a6d4b12",
   "metadata": {},
   "source": [
    "Performing Principal Component Analysis (PCA) on the wine quality dataset is a common technique to reduce the number of features while retaining most of the variance in the data. Here are the steps to perform PCA and determine the minimum number of principal components required to explain 90% of the variance:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the wine quality dataset (assuming you have it)\n",
    "wine_data = pd.read_csv('wine_quality.csv')\n",
    "\n",
    "# Separate the target variable (quality) from the features\n",
    "X = wine_data.drop('quality', axis=1)\n",
    "\n",
    "# Standardize the features (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Calculate the explained variance ratio for each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cumulative_explained_variance = explained_variance_ratio.cumsum()\n",
    "\n",
    "# Find the minimum number of principal components required to explain 90% of the variance\n",
    "n_components_90_percent = (cumulative_explained_variance >= 0.90).sum() + 1  # Add 1 because Python is 0-based\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of principal components required to explain 90% of variance: {n_components_90_percent}\")\n",
    "```\n",
    "\n",
    "In this code:\n",
    "\n",
    "1. We load the wine quality dataset and separate the target variable ('quality') from the features.\n",
    "\n",
    "2. Standardize the features using `StandardScaler`, which is important for PCA because it scales the features to have mean=0 and standard deviation=1.\n",
    "\n",
    "3. Perform PCA without specifying the number of components. This will calculate all the principal components.\n",
    "\n",
    "4. Calculate the explained variance ratio for each principal component. `explained_variance_ratio_` gives the proportion of variance explained by each component.\n",
    "\n",
    "5. Calculate the cumulative explained variance, which is the cumulative sum of explained variances.\n",
    "\n",
    "6. Determine the minimum number of principal components required to explain 90% of the variance by finding the index where the cumulative explained variance first exceeds or equals 0.90.\n",
    "\n",
    "7. Print the result, which will tell you the minimum number of principal components required to retain 90% of the variance in the data.\n",
    "\n",
    "Keep in mind that the choice of the number of principal components can also depend on the specific goals of your analysis and how much dimensionality reduction you are willing to accept while retaining sufficient information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155d88d-b0e3-4049-a97f-fa0669740bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
